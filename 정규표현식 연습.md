---
title: 프로젝트 1_ 기초
date: 2020-08-20
update: 2020-08-20
tags:
  - 프로젝트

---











# 정규표현식

1번:

```python
[0-9a-zA-Z]*@+?[0-9a-zA-Z]+\.+.*
```



2번:

```python
[가힣]*[^<a-zA-Z/"=>\s\.]\.*
```

* \s : 공백 문자



```python
[가힣]*[^<a-zA-Z/"=>\s\.]*\.*
```

* \* 붙으면 안 됨



3-1번: 

```python
<span[^>]*>(.*)</span>
```

* \b : 백스페이스바를 눌렀을 때 효과가 나타나게 됩니다. 백스페이스바 : 엔터키 위에 [←] 이런 기호가 적힌 키.
* '()': ()안에 있는 문자를 그룹화

3-2번: 







# re

* `re.match()`: 가장 첫음절에 대하여 검사
* `re.serch()`: 문자열 **전체**에 대하여 검사. 처음에 찾은 것만 return
* `re.findall()`: re.serch()를 for문 돌리지 않아도 전체 문장에 대하여 전체 단어를 return





# scrapy

* `cd workspace`: 프로젝트 할 때 쓸 작업 폴더 하나 생성하고 이걸로 다시 경로 defalt 생성
* `scrapy startproject naver_crawler`: scrapy 작업 할 naver_crawler 폴더 생성
* `cd naver_crawler`: 다시 경로 defalt 생성
* `ls`: 현재 디렉토리 뭐 있나 확인
* `scrapy crawl naver`: vs code에서 수정한대로 터미널에서 실행해줄 때



* 순서

  1. settings.py 파일 수정

     ```python
     ITEM_PIPELINES = {
        'naver_crawler.pipelines.NaverCrawlerPipeline': 300,
     }
     ```

     > 각주 처리 되어 있던 `ITEM_PIPELINES` 을 해제해준다.

  2. item.py 파일 수정

     ```python
     class NaverCrawlerItem(scrapy.Item):
         # define the fields for your item here like:
         url = scrapy.Field()
         media = scrapy.Field()
         content = scrapy.Field()
     ```

     > 크롤시 dic 형태로 저장할 변수 정의해준다.

  3. item.py 파일에서 정의한 변수대로 naver_spider.py 파일에 작성 & 구체적인 tag 추가한다.

     ```python
         def parse(self, response):
             # dic 형태로 쓸 때,
             item = NaverCrawlerItem()
             item['url'] = response.url
             item['content'] = response.xpath("//div[@id='articleBodyContents']//text()").getall()
             item['media'] = response.xpath("//div[@class='press_logo']/a/img/@alt").get() 
             # // : 모든 tag 검색 
     
             yield item
     ```

     > 이때 `item = NaverCrawlerItem()` 사용하므로 naver_spider.py에서 
     >
     > `from naver_crawler.items import NaverCrawlerItem` 써줘야 한다.
     >
     > * yield: iterable하게 item들이 하나씩 쌓인다. 
     >   이 code는 링크가 하나지만, 여러 개의 링크를 사용할 경우에는 램 용량이 터지는 걸 방지하기 위해 하나씩 넣어줘서 하나씩 불러오는 yield를 사용한다. 비록 '하나씩 넣어줘서 하나씩 불러오는 yield를 사용' 하지만 하나만 출력되는 print와 달리 전부 다 불러온다.
     >   * 더 추가적인 개념은 https://wikidocs.net/16069 참고

  4. pipelines.py의 `class NaverCrawlerPipeline:`에서 크롤링해서 불러올 txt 들을 정의 및 전처리 해준다.

     ```python
     from scrapy.exporters import CsvItemExporter
     ```

     > csv로 저장하기 위해 pipelines.py에서 해당 패키지를 작성해준다.

     ```python
     class NaverCrawlerPipeline:
         def process_item(self, item, spider):
             with open('news.csv', 'wb') as f: # wb: 바이너리
                 exporter = CsvItemExporter(f, encoding="utf-8")
                 item['content'] = str(item['content']).replace("// flash 오류를 우회하기 위한 함수 추가", "")
                 exporter.export_item(item)
             return itemscrapy crawl naver
     ```

     > 위의 `.replace("// flash 오류를 우회하기 위한 함수 추가", "")`처럼 여기서 전처리 가능하다.
     >
     > 그런데 보통은 전처리 없이 raw data만을 가져와서 pandas에서 csv 읽고, 거기서 한꺼번에 전처리 하는 방식이 더 낫다.
     >
     > * 크롤링이 cpu를 많이 잡아먹기 때문.

  5. terminal에서 `scrapy crawl naver`작성 및 실행하여 크롤링 잘 되는지 확인한다.



# VSCODE

* 가상환경 빌드

  * 2.대 혹은 특정 version의 python이 필요할 때 사용

  ```python
  conda create --name mulcam python=3.6 anaconda
  ```

  
